{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful soup --> 11 Oct\n",
    "\n",
    "\n",
    "https://www.w3.org/TR/PNG/iso_8859-1.txt\n",
    "\n",
    "https://www.example.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from utils.config import Config\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get() --> opens a network, gets status \n",
    "# status_code --> current status\n",
    "# headers\n",
    "# text --> extract text\n",
    "# json --> extract json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.example.com\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "\n",
    "    print(html_content[:500])\n",
    "\n",
    "else:\n",
    "    print('failed with status: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['weather_map_key'])\n",
      "2b39533aa7d657dbc2c3f16b18733d88\n"
     ]
    }
   ],
   "source": [
    "# Config.load_config('jsonfile/keys.json')\n",
    "\n",
    "\n",
    "# print(Config.get_key('weather_map_key'))\n",
    "\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(config.get_key_names())\n",
    "\n",
    "\n",
    "\n",
    "api_key = config.get_key('weather_map_key')\n",
    "\n",
    "print(api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'Mumbai'\n",
    "lat = 18.5204\n",
    "lon = 73.8567\n",
    "url = f'https://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    weather_data = response.json()\n",
    "    #city_name = weather_data[\"name\"]\n",
    "else:\n",
    "    print(f\"failed with status: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coord': {'lon': 72.8479, 'lat': 19.0144},\n",
       " 'weather': [{'id': 211,\n",
       "   'main': 'Thunderstorm',\n",
       "   'description': 'thunderstorm',\n",
       "   'icon': '11n'}],\n",
       " 'base': 'stations',\n",
       " 'main': {'temp': 300.14,\n",
       "  'feels_like': 302.33,\n",
       "  'temp_min': 300.14,\n",
       "  'temp_max': 301.09,\n",
       "  'pressure': 1009,\n",
       "  'humidity': 74,\n",
       "  'sea_level': 1009,\n",
       "  'grnd_level': 1007},\n",
       " 'visibility': 2500,\n",
       " 'wind': {'speed': 4.63, 'deg': 50, 'gust': 9.77},\n",
       " 'clouds': {'all': 75},\n",
       " 'dt': 1728912528,\n",
       " 'sys': {'type': 1,\n",
       "  'id': 9052,\n",
       "  'country': 'IN',\n",
       "  'sunrise': 1728867737,\n",
       "  'sunset': 1728909992},\n",
       " 'timezone': 19800,\n",
       " 'id': 1275339,\n",
       " 'name': 'Mumbai',\n",
       " 'cod': 200}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url ='https://news.ycombinator.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.Upgrading Uber's MySQL Fleet (uber.com) | None\n",
      "1.Show HN: I made a git rebase TUI editor (github.com/nyub) | None\n",
      "2.Exploring 120 Years of Timezones (2021) (scottlogic.com) | None\n",
      "3.Huly â€“ Open-source project management platform (github.com/hcengineering) | None\n",
      "4.A Music Map of every genre and subgenre (musicmap.info) | None\n",
      "5.Show HN: X11 tool to share a screen area in any video meeting (github.com/splitbrain) | None\n",
      "6.Curly-Cue: Geometric Methods for Highly Coiled Hair (yale.edu) | None\n",
      "7.Starship Flight 5: Launch and booster catch [video] (twitter.com/spacex) | None\n",
      "8.Python client for the $20 Colmi R02 smart ring (tahnok.github.io) | None\n",
      "9.The quiet art of attention (billwear.github.io) | None\n",
      "10.Counterintuitive Properties of High Dimensional Space (2018) (eecs.berkeley.edu) | None\n",
      "11.Refurb weekend: the Symbolics MacIvory Lisp machine I have hated (oldvcr.blogspot.com) | None\n",
      "12.Making the Tibetan language a first-class citizen in the digital world (bdrc.io) | None\n",
      "13.Show HN: A VSCode Extension to edit HTML visually in real-time (github.com/urin) | None\n",
      "14.Faster convergence for diffusion models (sihyun.me) | None\n",
      "15.Why does FM sound better than AM? (johndcook.com) | None\n",
      "16.C++ String Conversion: Exploring std:from_chars in C++17 to C++26 (cppstories.com) | None\n",
      "17.Rama on Clojure's terms, and the magic of continuation-passing style (redplanetlabs.com) | None\n",
      "18.Transforming Colors with Matrices (lisyarus.github.io) | None\n",
      "19.In a rare disclosure, The Pentagon provides an update on the X-37B spaceplane (arstechnica.com) | None\n",
      "20.Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel 2024 (nobelprize.org) | None\n",
      "21.Christopher Columbus may have been Spanish and Jewish, documentary says (theguardian.com) | None\n",
      "22.Avoiding a Geopolitical open-source Apocalypse (thenewstack.io) | None\n",
      "23.Honeybee: Calling via XMPP (momi.ca) | None\n",
      "24.Introducing Our New Name (minetest.net) | None\n",
      "25.Common Lisp implementation of the Forth 2012 Standard (github.com/gmpalter) | None\n",
      "26.A review after using Rust on embedded in production for over a year (lohr.dev) | None\n",
      "27.The Inevitability of Mixing Open Source and Money (pocoo.org) | None\n",
      "28.Node.js, Pipes, and Disappearing Bytes (sxlijin.github.io) | None\n",
      "29.All asteroids in Solar System, visualized (github.com/darkstar1982) | None\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    #html_content = response.text\n",
    "    soup = BeautifulSoup(response.text,'html.parser')  #html code\n",
    "    titles = soup.find_all('span', class_='titleline' )\n",
    "    for index, title in enumerate(titles):\n",
    "        title_text = title.text # get text of title\n",
    "        title_link = title.get('href') # get href or URL\n",
    "        print(f'{index}.{title_text} | {title_link}')\n",
    "\n",
    "else:\n",
    "    print('failed with status: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "\n",
    "# url = f''\n",
    "# url = f'https://api.unsplash.com/photos/random?query=minimal'\n",
    "\n",
    "# response = requests.get(url)\n",
    "# if response.status_code == 200:\n",
    "#     image_data = response.json()\n",
    "#     image_url = image_data['urls']['regular']  # Get the URL of the image\n",
    "#     print(f\"Image URL: {image_url}\")\n",
    "\n",
    "#     # Display the image in the Jupyter Notebook\n",
    "#     display(Image(url=image_url))\n",
    "    \n",
    "    \n",
    "# else:\n",
    "#     print(f\"failed with status: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the page: Web scraping - Wikipedia\n",
      "\n",
      "All links (first 5):\n",
      "Text: Jump to content, URL: #bodyContent\n",
      "Text: Main page, URL: /wiki/Main_Page\n",
      "Text: Contents, URL: /wiki/Wikipedia:Contents\n",
      "Text: Current events, URL: /wiki/Portal:Current_events\n",
      "Text: Random article, URL: /wiki/Special:Random\n",
      "\n",
      "First Paragraph text: Web scraping,web harvesting, orweb data extractionisdata scrapingused forextracting datafromwebsites.[1]Web scraping software may directly access theWorld Wide Webusing theHypertext Transfer Protocolor a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using abotorweb crawler. It is a form of copying in which specific data is gathered and copied from the web, typically into a central localdatabaseorspreadsheet, for laterretrievaloranalysis.\n",
      "\n",
      "No Table of Contents found.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "\n",
    "# Send a GET request to fetch the raw HTML content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the page content with BeautifulSoup using the 'lxml' parser\n",
    "soup = BeautifulSoup(response.content, 'lxml') \n",
    "\n",
    "# 1. Extract the title of the page\n",
    "title = soup.title.string \n",
    "print(f\"Title of the page: {title}\")\n",
    "\n",
    "# 2. Extract all the anchor tags (links)\n",
    "all_links = soup.find_all('a',href=True)\n",
    "print(\"\\nAll links (first 5):\")\n",
    "for link in all_links[:5]:   #Displaying only first 5 links for brevity\n",
    "    print(f\"Text: {link.get_text()}, URL: {link['href']}\") \n",
    "\n",
    "# 3. Extract the first paragraph text\n",
    "first_paragraph = soup.find('p').get_text(strip=True) \n",
    "print(f\"\\nFirst Paragraph text: {first_paragraph}\")\n",
    "\n",
    "# 4. Extract the sidebar table of contents (if present)\n",
    "toc = soup.find('div',class_='toc')\n",
    "if toc:\n",
    "    toc_text = toc.get_text(separator=\"\\n\",strip=True)\n",
    "    print(f\"\\n Table of contents: \\n{toc_text}\")\n",
    "else:\n",
    "    print(\"\\nNo Table of Contents found.\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDir='images/'\n",
    "\n",
    "def download_image(url):\n",
    "    photo_name = url.split('?')[0]\n",
    "    photo_name = photo_name.split('/')[-1]\n",
    "    photo_name = os.path.join(outDir, f'{photo_name}.png')\n",
    "    response = requests.get(url)\n",
    "    with open(photo_name,'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 images for the query 'jordan':\n",
      "\n",
      "Image 1: https://images.unsplash.com/photo-1532336414038-cf19250c5757?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MDB8MHwxfHNlYXJjaHwxfHxqb3JkYW58ZW58MHx8fHwxNzI4OTE0NTQwfDA&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 2: https://images.unsplash.com/photo-1574786527860-f2e274867c91?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MDB8MHwxfHNlYXJjaHwyfHxqb3JkYW58ZW58MHx8fHwxNzI4OTE0NTQwfDA&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 3: https://images.unsplash.com/photo-1547234935-80c7145ec969?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MDB8MHwxfHNlYXJjaHwzfHxqb3JkYW58ZW58MHx8fHwxNzI4OTE0NTQwfDA&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 4: https://images.unsplash.com/photo-1492693429561-1c283eb1b2e8?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MDB8MHwxfHNlYXJjaHw0fHxqb3JkYW58ZW58MHx8fHwxNzI4OTE0NTQwfDA&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 5: https://images.unsplash.com/photo-1548786811-dd6e453ccca7?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MDB8MHwxfHNlYXJjaHw1fHxqb3JkYW58ZW58MHx8fHwxNzI4OTE0NTQwfDA&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 6: https://images.unsplash.com/photo-1504765863531-de3980d06137?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MDB8MHwxfHNlYXJjaHw2fHxqb3JkYW58ZW58MHx8fHwxNzI4OTE0NTQwfDA&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 7: https://images.unsplash.com/photo-1547234936-74a4b1ee7f42?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MDB8MHwxfHNlYXJjaHw3fHxqb3JkYW58ZW58MHx8fHwxNzI4OTE0NTQwfDA&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 8: https://images.unsplash.com/photo-1551171128-c5b124a4174c?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MDB8MHwxfHNlYXJjaHw4fHxqb3JkYW58ZW58MHx8fHwxNzI4OTE0NTQwfDA&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 9: https://images.unsplash.com/photo-1606210122158-eeb10e0823bf?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MDB8MHwxfHNlYXJjaHw5fHxqb3JkYW58ZW58MHx8fHwxNzI4OTE0NTQwfDA&ixlib=rb-4.0.3&q=80&w=1080\n",
      "Image 10: https://images.unsplash.com/photo-1501232060322-aa87215ab531?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w2NjQ3MDB8MHwxfHNlYXJjaHwxMHx8am9yZGFufGVufDB8fHx8MTcyODkxNDU0MHww&ixlib=rb-4.0.3&q=80&w=1080\n"
     ]
    }
   ],
   "source": [
    "access_key = \"pIMbHUcsCJmTxLs74uc4YWD0gkr71gYPAqXdeFvRkOw\"\n",
    "query = 'jordan'\n",
    "\n",
    "# Make a GET request to the unsplash api  \n",
    "url = f\"https://api.unsplash.com/search/photos?query={query}&client_id={access_key}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "\n",
    "# Extract image URLs from the response\n",
    "if 'results' in data:\n",
    "    print(f\"{len(data['results'])} images for the query '{query}':\\n\")\n",
    "    for i, photo in enumerate(data['results']):\n",
    "        image_url = photo['urls']['regular']\n",
    "        print(f\"Image {i+1}: {image_url}\")\n",
    "        download_image(image_url)\n",
    "\n",
    "else:\n",
    "    print(\"No results found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sep2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
